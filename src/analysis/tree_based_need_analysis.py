"""
========================================
ìì‚´ë¥ -Need ì§€í‘œ ë™ë°˜ì„± ë¶„ì„
========================================
ëª©ì : ìì‚´ë¥ ì´ ë†’ì€ ì§€ì—­ì—ì„œ í•¨ê»˜ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” Need ì§€í‘œ íƒìƒ‰
ë°©ë²•: RandomForest + SHAP
ì£¼ì˜: 
  - ì¸ê³¼ê´€ê³„ ì¶”ë¡  ë¶ˆê°€ (n=25, ì†Œí‘œë³¸)
  - ì˜ˆì¸¡ ì„±ëŠ¥ ê²½ìŸ ëª©ì  ì•„ë‹˜
  - "ë™ë°˜ì„±" ë° "êµ¬ì¡°ì  íŒ¨í„´" íŒŒì•…ìš©
========================================
"""

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import shap
import warnings
warnings.filterwarnings('ignore')

# ========================================
# 1. ê²½ë¡œ ì„¤ì •
# ========================================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, 'data')
INPUT_PATH = os.path.join(DATA_DIR, 'processed', 'need_tidy.csv')
OUTPUT_DIR = os.path.join(DATA_DIR, 'outputs', 'model')

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("="*60)
print("ìì‚´ë¥ -Need ì§€í‘œ ë™ë°˜ì„± ë¶„ì„ (RandomForest + SHAP)")
print("="*60)
print(f"ì…ë ¥ íŒŒì¼: {INPUT_PATH}")
print(f"ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")
print("="*60)

# ========================================
# 2. ë°ì´í„° ë¡œë“œ
# ========================================
df = pd.read_csv(INPUT_PATH)
print(f"\nâœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]}ê°œ ìì¹˜êµ¬, {df.shape[1]}ê°œ ë³€ìˆ˜")

# ========================================
# 3. ë³€ìˆ˜ ë¶„ë¦¬
# ========================================
# Target
y = df['suicide_rate'].values

# Features (district ì œì™¸)
feature_cols = [col for col in df.columns 
                if col not in ['district', 'suicide_rate']]
X = df[feature_cols].values

print(f"\nâœ“ Target: suicide_rate")
print(f"âœ“ Features ({len(feature_cols)}ê°œ):")
for i, col in enumerate(feature_cols, 1):
    print(f"   {i}. {col}")

# ========================================
# 4. RandomForest í•™ìŠµ
# ========================================
# ì†Œí‘œë³¸ íŠ¹ì„±ìƒ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
# n=25ì´ë¯€ë¡œ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ max_depth ì œí•œ
RF_PARAMS = {
    'n_estimators': 300,
    'max_depth': 4,  # ì†Œí‘œë³¸ì´ë¯€ë¡œ ê¹Šì´ ì œí•œ
    'min_samples_split': 3,
    'min_samples_leaf': 2,
    'max_features': 'sqrt',
    'random_state': 42,
    'n_jobs': -1
}

print(f"\n{'='*60}")
print("RandomForest í•™ìŠµ ì‹œì‘")
print(f"{'='*60}")
for key, val in RF_PARAMS.items():
    print(f"  {key}: {val}")

rf_model = RandomForestRegressor(**RF_PARAMS)
rf_model.fit(X, y)

# ========================================
# 5. ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
# ========================================
y_pred = rf_model.predict(X)
train_r2 = r2_score(y, y_pred)
train_rmse = np.sqrt(mean_squared_error(y, y_pred))

# Cross-validation (5-fold, ì†Œí‘œë³¸ì´ë¯€ë¡œ 5-fold)
cv_scores = cross_val_score(rf_model, X, y, cv=5, 
                            scoring='r2', n_jobs=-1)
cv_r2_mean = cv_scores.mean()
cv_r2_std = cv_scores.std()

print(f"\n{'='*60}")
print("ëª¨ë¸ ì„±ëŠ¥")
print(f"{'='*60}")
print(f"  Train RÂ²:  {train_r2:.4f}")
print(f"  Train RMSE: {train_rmse:.4f}")
print(f"  CV RÂ² (5-fold): {cv_r2_mean:.4f} (Â±{cv_r2_std:.4f})")
print(f"\nâš ï¸  ì£¼ì˜: n=25 ì†Œí‘œë³¸ì´ë¯€ë¡œ ì„±ëŠ¥ ì§€í‘œëŠ” ì°¸ê³ ìš©")
print(f"    â†’ ì˜ˆì¸¡ ì„±ëŠ¥ë³´ë‹¤ 'ë³€ìˆ˜ ê°„ ë™ë°˜ì„± íŒ¨í„´' íŒŒì•…ì´ ëª©ì ")
print(f"{'='*60}")

# ========================================
# 6. Feature Importance ì¶”ì¶œ
# ========================================
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_model.feature_importances_
})
importance_df = importance_df.sort_values('importance', ascending=False)
importance_df['importance_rank'] = range(1, len(importance_df) + 1)

# ì €ì¥
fi_path = os.path.join(OUTPUT_DIR, 'rf_feature_importance.csv')
importance_df.to_csv(fi_path, index=False, encoding='utf-8-sig')

print(f"\nâœ“ Feature Importance ì €ì¥: {fi_path}")
print("\n[Feature Importance Top 5]")
print(importance_df.head().to_string(index=False))

# ========================================
# 7. SHAP ë¶„ì„
# ========================================
print(f"\n{'='*60}")
print("SHAP ë¶„ì„ ì‹œì‘ (TreeExplainer)")
print(f"{'='*60}")

explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X)

# SHAP ì ˆëŒ€ê°’ í‰ê·  ê³„ì‚°
shap_summary = pd.DataFrame({
    'feature': feature_cols,
    'mean_abs_shap_value': np.abs(shap_values).mean(axis=0)
})
shap_summary = shap_summary.sort_values('mean_abs_shap_value', ascending=False)
shap_summary['shap_rank'] = range(1, len(shap_summary) + 1)

# ì €ì¥
shap_path = os.path.join(OUTPUT_DIR, 'rf_shap_summary.csv')
shap_summary.to_csv(shap_path, index=False, encoding='utf-8-sig')

print(f"\nâœ“ SHAP Summary ì €ì¥: {shap_path}")
print("\n[SHAP Importance Top 5]")
print(shap_summary.head().to_string(index=False))

# ========================================
# 8. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
# ========================================
predictions_df = pd.DataFrame({
    'district': df['district'],
    'suicide_rate_actual': y,
    'suicide_rate_predicted': y_pred,
    'residual': y - y_pred
})

pred_path = os.path.join(OUTPUT_DIR, 'rf_predictions.csv')
predictions_df.to_csv(pred_path, index=False, encoding='utf-8-sig')

print(f"\nâœ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {pred_path}")

# ========================================
# 9. ê²°ê³¼ í•´ì„ ê°€ì´ë“œ
# ========================================
print(f"\n{'='*60}")
print("ğŸ“Š ê²°ê³¼ í•´ì„ ê°€ì´ë“œ (ì•ˆì „í•œ í‘œí˜„)")
print(f"{'='*60}")
print("""
âœ… ì˜¬ë°”ë¥¸ í•´ì„ (ë™ë°˜ì„±/íŒ¨í„´):
  â€¢ "ìì‚´ë¥ ì´ ë†’ì€ ìì¹˜êµ¬ì—ì„œ {ë³€ìˆ˜ëª…}ë„ í•¨ê»˜ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥"
  â€¢ "{ë³€ìˆ˜ëª…}ì€ ìì‚´ë¥  ë³€ë™ê³¼ ê°•í•œ ë™ë°˜ì„±ì„ ë³´ì„"
  â€¢ "RandomForest ëª¨ë¸ì´ ìì‚´ë¥  íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° {ë³€ìˆ˜ëª…}ì„ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œ í™œìš©"
  â€¢ "SHAP ë¶„ì„ ê²°ê³¼, {ë³€ìˆ˜ëª…}ì´ ì˜ˆì¸¡ì— ê°€ì¥ í° ê¸°ì—¬"
  
âŒ í”¼í•´ì•¼ í•  í•´ì„ (ì¸ê³¼/ì •ì±…):
  â€¢ "{ë³€ìˆ˜ëª…}ì´ ìì‚´ë¥ ì„ ì¦ê°€/ê°ì†Œì‹œí‚¨ë‹¤" â†’ ì¸ê³¼ê´€ê³„ ì£¼ì¥ ë¶ˆê°€
  â€¢ "{ë³€ìˆ˜ëª…}ì„ ê°œì„ í•˜ë©´ ìì‚´ë¥ ì´ ë‚®ì•„ì§„ë‹¤" â†’ ì •ì±… íš¨ê³¼ ì¶”ì • ë¶ˆê°€
  â€¢ "ì´ ëª¨ë¸ë¡œ ë¯¸ë˜ ìì‚´ë¥  ì˜ˆì¸¡ ê°€ëŠ¥" â†’ n=25, ì˜ˆì¸¡ ëª©ì  ì•„ë‹˜
  
ğŸ” ë§¥ë½:
  - n=25 (ì„œìš¸ì‹œ ìì¹˜êµ¬) ì†Œí‘œë³¸ â†’ í†µê³„ì  ì¼ë°˜í™” ì œí•œì 
  - ì§€ì—­ ë‹¨ìœ„ ì§‘ê³„ ë°ì´í„° â†’ ìƒíƒœí•™ì  ì˜¤ë¥˜(ecological fallacy) ê°€ëŠ¥ì„±
  - íŠ¸ë¦¬ ëª¨ë¸ íŠ¹ì„±ìƒ ë¹„ì„ í˜•/ìƒí˜¸ì‘ìš© íŒ¨í„´ í¬ì°©
  - ë³¸ ë¶„ì„ì€ 'íƒìƒ‰ì (exploratory)' ì„±ê²©
""")

print(f"\n{'='*60}")
print("âœ… ë¶„ì„ ì™„ë£Œ")
print(f"{'='*60}")
print(f"ì €ì¥ëœ íŒŒì¼:")
print(f"  1. {fi_path}")
print(f"  2. {shap_path}")
print(f"  3. {pred_path}")
print(f"{'='*60}\n")

# ========================================
# 10. ìš”ì•½ í†µê³„ ì¶œë ¥
# ========================================
print("ğŸ“ˆ ìš”ì•½ í†µê³„")
print(f"{'='*60}")
print(f"ìµœê³  ì¤‘ìš”ë„ ë³€ìˆ˜ (Feature Importance):")
print(f"  â†’ {importance_df.iloc[0]['feature']}")
print(f"     (importance: {importance_df.iloc[0]['importance']:.4f})")
print(f"\nìµœê³  ì¤‘ìš”ë„ ë³€ìˆ˜ (SHAP):")
print(f"  â†’ {shap_summary.iloc[0]['feature']}")
print(f"     (mean |SHAP|: {shap_summary.iloc[0]['mean_abs_shap_value']:.4f})")
print(f"{'='*60}\n")
"""
========================================
ìì‚´ë¥ -Need ì§€í‘œ ë™ë°˜ì„± ë¶„ì„
========================================

ëª©ì :
- ìì‚´ë¥ ì´ ë†’ì€ ì§€ì—­ì—ì„œ
  ì–´ë–¤ Need ì§€í‘œë“¤ì´ í•¨ê»˜ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ íƒìƒ‰
- "ì›ì¸ ê·œëª…"ì´ ì•„ë‹ˆë¼,
  ì§€ì—­ ë‹¨ìœ„ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ê´€ì°°ë˜ëŠ”
  êµ¬ì¡°ì  ë™ë°˜ íŒ¨í„´(co-occurrence)ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ëª©ì 

ë°©ë²•:
- RandomForest íšŒê·€ ëª¨ë¸
- SHAPì„ ì´ìš©í•œ ë³€ìˆ˜ ê¸°ì—¬ë„ í•´ì„

ì£¼ì˜:
- ì¸ê³¼ê´€ê³„ ì¶”ë¡  ë¶ˆê°€ (n=25, ì†Œí‘œë³¸)
- ì˜ˆì¸¡ ì„±ëŠ¥ ê²½ìŸ ëª©ì  ì•„ë‹˜
- ì •ì±… íŒë‹¨ì„ ìœ„í•œ 'ì„¤ëª… ê°€ëŠ¥í•œ íƒìƒ‰ ë¶„ì„'
========================================
"""

import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import shap
import warnings
warnings.filterwarnings('ignore')

# =====================================================
# 1. ê²½ë¡œ ì„¤ì •
# =====================================================
# ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
# í”„ë¡œì íŠ¸ ë£¨íŠ¸(BASE_DIR)ë¥¼ ê³„ì‚°
# (íŒŒì¼ êµ¬ì¡°ê°€ ë°”ë€Œì–´ë„ ìƒëŒ€ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë™ì‘)
BASE_DIR = os.path.dirname(
    os.path.dirname(
        os.path.dirname(os.path.abspath(__file__))
    )
)

# data ë””ë ‰í† ë¦¬
DATA_DIR = os.path.join(BASE_DIR, 'data')

# ì…ë ¥ ë°ì´í„°:
# - need_tidy.csv
# - district ë‹¨ìœ„ Need ì§€í‘œ + suicide_rate í¬í•¨
INPUT_PATH = os.path.join(DATA_DIR, 'processed', 'need_tidy.csv')

# ì¶œë ¥ ë””ë ‰í† ë¦¬:
# - ëª¨ë¸ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ ì €ì¥
OUTPUT_DIR = os.path.join(DATA_DIR, 'outputs', 'model')

# ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ë¶„ì„ ì‹œì‘ ë¡œê·¸
print("=" * 60)
print("ìì‚´ë¥ -Need ì§€í‘œ ë™ë°˜ì„± ë¶„ì„ (RandomForest + SHAP)")
print("=" * 60)
print(f"ì…ë ¥ íŒŒì¼: {INPUT_PATH}")
print(f"ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")
print("=" * 60)

# =====================================================
# 2. ë°ì´í„° ë¡œë“œ
# =====================================================
# Need ì§€í‘œì™€ ìì‚´ë¥ ì´ í¬í•¨ëœ ë°ì´í„° ë¡œë“œ
df = pd.read_csv(INPUT_PATH)

# í‘œë³¸ í¬ê¸° ë° ë³€ìˆ˜ ê°œìˆ˜ í™•ì¸
print(f"\nâœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]}ê°œ ìì¹˜êµ¬, {df.shape[1]}ê°œ ë³€ìˆ˜")

# =====================================================
# 3. ë³€ìˆ˜ ë¶„ë¦¬
# =====================================================
# Target ë³€ìˆ˜:
# - suicide_rate
# - ì„¤ëª… ëŒ€ìƒì´ ë˜ëŠ” ê²°ê³¼ ì§€í‘œ
y = df['suicide_rate'].values

# Feature ë³€ìˆ˜:
# - districtì™€ suicide_rateë¥¼ ì œì™¸í•œ ëª¨ë“  Need ì§€í‘œ
# - ì¦‰, 'ìì‚´ë¥ ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ í•¨ê»˜ ê´€ì°°ë˜ëŠ” ìœ„í—˜ ìš”ì¸ë“¤'
feature_cols = [
    col for col in df.columns
    if col not in ['district', 'suicide_rate']
]
X = df[feature_cols].values

print(f"\nâœ“ Target: suicide_rate")
print(f"âœ“ Features ({len(feature_cols)}ê°œ):")
for i, col in enumerate(feature_cols, 1):
    print(f"   {i}. {col}")

# =====================================================
# 4. RandomForest í•™ìŠµ
# =====================================================
# ì†Œí‘œë³¸(n=25) íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬
# ëª¨ë¸ ë³µì¡ë„ë¥¼ ë³´ìˆ˜ì ìœ¼ë¡œ ì œí•œ
#
# ì´ ëª¨ë¸ì˜ ì—­í• :
# - 'ìì‚´ë¥ ì„ ì˜ ë§íˆëŠ” ì˜ˆì¸¡ê¸°' âŒ
# - 'ìì‚´ë¥  íŒ¨í„´ì„ ì„¤ëª…í•˜ëŠ” ë° ìì£¼ í™œìš©ë˜ëŠ” ë³€ìˆ˜ êµ¬ì¡°' â­•
RF_PARAMS = {
    'n_estimators': 300,       # íŠ¸ë¦¬ ê°œìˆ˜ â†’ ë¶„ì‚° ê°ì†Œ ëª©ì 
    'max_depth': 4,            # ê¹Šì´ ì œí•œ â†’ ê³¼ì í•© ë°©ì§€
    'min_samples_split': 3,
    'min_samples_leaf': 2,
    'max_features': 'sqrt',    # ëœë¤ì„± ê°•í™” â†’ ì•ˆì •ì„± ì¦ê°€
    'random_state': 42,
    'n_jobs': -1
}

print(f"\n{'=' * 60}")
print("RandomForest í•™ìŠµ ì‹œì‘")
print(f"{'=' * 60}")
for key, val in RF_PARAMS.items():
    print(f"  {key}: {val}")

# ëª¨ë¸ í•™ìŠµ
rf_model = RandomForestRegressor(**RF_PARAMS)
rf_model.fit(X, y)

# =====================================================
# 5. ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€ (ì°¸ê³ ìš©)
# =====================================================
# í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
y_pred = rf_model.predict(X)

# í•™ìŠµ ë°ì´í„° ê¸°ì¤€ ì„±ëŠ¥
# â†’ ê³¼ì í•© ì—¬ë¶€ë¥¼ ê°€ë³ê²Œ í™•ì¸í•˜ëŠ” ì°¸ê³  ì§€í‘œ
train_r2 = r2_score(y, y_pred)
train_rmse = np.sqrt(mean_squared_error(y, y_pred))

# 5-fold êµì°¨ê²€ì¦
# - ì†Œí‘œë³¸ì´ë¯€ë¡œ fold ìˆ˜ë¥¼ í¬ê²Œ ì¡ì§€ ì•ŠìŒ
# - ì„±ëŠ¥ ìì²´ë³´ë‹¤ ë³€ë™ì„± í™•ì¸ ëª©ì 
cv_scores = cross_val_score(
    rf_model,
    X,
    y,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

cv_r2_mean = cv_scores.mean()
cv_r2_std = cv_scores.std()

print(f"\n{'=' * 60}")
print("ëª¨ë¸ ì„±ëŠ¥ (ì°¸ê³ ìš©)")
print(f"{'=' * 60}")
print(f"  Train RÂ²:  {train_r2:.4f}")
print(f"  Train RMSE: {train_rmse:.4f}")
print(f"  CV RÂ² (5-fold): {cv_r2_mean:.4f} (Â±{cv_r2_std:.4f})")
print(f"\nâš ï¸ ì£¼ì˜: n=25 ì†Œí‘œë³¸ì´ë¯€ë¡œ ì„±ëŠ¥ ì§€í‘œëŠ” ì°¸ê³ ìš©")
print(f"    â†’ ì˜ˆì¸¡ ì„±ëŠ¥ë³´ë‹¤ 'ë³€ìˆ˜ ê°„ ë™ë°˜ì„± íŒ¨í„´' íŒŒì•…ì´ ëª©ì ")
print(f"{'=' * 60}")

# =====================================================
# 6. Feature Importance ì¶”ì¶œ
# =====================================================
# RandomForest ë‚´ë¶€ ê¸°ì¤€(Gini/ë¶„ì‚° ê°ì†Œ)ì— ë”°ë¥¸ ì¤‘ìš”ë„
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_model.feature_importances_
})

# ì¤‘ìš”ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
importance_df = importance_df.sort_values(
    'importance',
    ascending=False
)
importance_df['importance_rank'] = range(
    1, len(importance_df) + 1
)

# ì €ì¥
fi_path = os.path.join(OUTPUT_DIR, 'rf_feature_importance.csv')
importance_df.to_csv(
    fi_path,
    index=False,
    encoding='utf-8-sig'
)

print(f"\nâœ“ Feature Importance ì €ì¥: {fi_path}")
print("\n[Feature Importance Top 5]")
print(importance_df.head().to_string(index=False))

# =====================================================
# 7. SHAP ë¶„ì„
# =====================================================
# ëª©ì :
# - ë‹¨ìˆœ ì¤‘ìš”ë„ ìˆœìœ„ê°€ ì•„ë‹ˆë¼,
#   ê° ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— 'ì–´ëŠ ë°©í–¥ìœ¼ë¡œ, ì–¼ë§ˆë‚˜' ê¸°ì—¬í–ˆëŠ”ì§€ í•´ì„
print(f"\n{'=' * 60}")
print("SHAP ë¶„ì„ ì‹œì‘ (TreeExplainer)")
print(f"{'=' * 60}")

# íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì´ë¯€ë¡œ TreeExplainer ì‚¬ìš©
explainer = shap.TreeExplainer(rf_model)

# ê° ìƒ˜í”Œ Ã— ê° ë³€ìˆ˜ì— ëŒ€í•œ SHAP ê°’
shap_values = explainer.shap_values(X)

# SHAP ì ˆëŒ€ê°’ í‰ê· :
# - ì „ì²´ ìì¹˜êµ¬ì—ì„œ í•´ë‹¹ ë³€ìˆ˜ê°€
#   ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ í¬ê²Œ ê¸°ì—¬í–ˆëŠ”ì§€ ìš”ì•½
shap_summary = pd.DataFrame({
    'feature': feature_cols,
    'mean_abs_shap_value': np.abs(shap_values).mean(axis=0)
})

shap_summary = shap_summary.sort_values(
    'mean_abs_shap_value',
    ascending=False
)
shap_summary['shap_rank'] = range(
    1, len(shap_summary) + 1
)

# ì €ì¥
shap_path = os.path.join(OUTPUT_DIR, 'rf_shap_summary.csv')
shap_summary.to_csv(
    shap_path,
    index=False,
    encoding='utf-8-sig'
)

print(f"\nâœ“ SHAP Summary ì €ì¥: {shap_path}")
print("\n[SHAP Importance Top 5]")
print(shap_summary.head().to_string(index=False))

# =====================================================
# 8. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (ì”ì°¨ ë¶„ì„ìš©)
# =====================================================
# ì”ì°¨(residual):
# - ì‹¤ì œ ìì‚´ë¥  - ì˜ˆì¸¡ ìì‚´ë¥ 
# - ëª¨ë¸ íŒ¨í„´ìœ¼ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ì§€ì—­ íŠ¹ì´ì„± íƒìƒ‰ ê°€ëŠ¥
predictions_df = pd.DataFrame({
    'district': df['district'],
    'suicide_rate_actual': y,
    'suicide_rate_predicted': y_pred,
    'residual': y - y_pred
})

pred_path = os.path.join(OUTPUT_DIR, 'rf_predictions.csv')
predictions_df.to_csv(
    pred_path,
    index=False,
    encoding='utf-8-sig'
)

print(f"\nâœ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {pred_path}")

# =====================================================
# 9. ê²°ê³¼ í•´ì„ ê°€ì´ë“œ (ì¤‘ìš”)
# =====================================================
print(f"\n{'=' * 60}")
print("ğŸ“Š ê²°ê³¼ í•´ì„ ê°€ì´ë“œ (ì•ˆì „í•œ í‘œí˜„)")
print(f"{'=' * 60}")
print("""
âœ… ì˜¬ë°”ë¥¸ í•´ì„ (ë™ë°˜ì„±/íŒ¨í„´):
  â€¢ "ìì‚´ë¥ ì´ ë†’ì€ ìì¹˜êµ¬ì—ì„œ {ë³€ìˆ˜ëª…}ë„ í•¨ê»˜ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥"
  â€¢ "{ë³€ìˆ˜ëª…}ì€ ìì‚´ë¥  ë³€ë™ê³¼ ê°•í•œ ë™ë°˜ì„±ì„ ë³´ì„"
  â€¢ "RandomForest ëª¨ë¸ì´ ìì‚´ë¥  íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° {ë³€ìˆ˜ëª…}ì„ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œ í™œìš©"
  â€¢ "SHAP ë¶„ì„ ê²°ê³¼, {ë³€ìˆ˜ëª…}ì´ ì˜ˆì¸¡ì— ê°€ì¥ í° ê¸°ì—¬"

âŒ í”¼í•´ì•¼ í•  í•´ì„ (ì¸ê³¼/ì •ì±…):
  â€¢ "{ë³€ìˆ˜ëª…}ì´ ìì‚´ë¥ ì„ ì¦ê°€/ê°ì†Œì‹œí‚¨ë‹¤" â†’ ì¸ê³¼ê´€ê³„ ì£¼ì¥ ë¶ˆê°€
  â€¢ "{ë³€ìˆ˜ëª…}ì„ ê°œì„ í•˜ë©´ ìì‚´ë¥ ì´ ë‚®ì•„ì§„ë‹¤" â†’ ì •ì±… íš¨ê³¼ ì¶”ì • ë¶ˆê°€
  â€¢ "ì´ ëª¨ë¸ë¡œ ë¯¸ë˜ ìì‚´ë¥  ì˜ˆì¸¡ ê°€ëŠ¥" â†’ n=25, ì˜ˆì¸¡ ëª©ì  ì•„ë‹˜

ğŸ” ë§¥ë½:
  - n=25 (ì„œìš¸ì‹œ ìì¹˜êµ¬) ì†Œí‘œë³¸ â†’ í†µê³„ì  ì¼ë°˜í™” ì œí•œì 
  - ì§€ì—­ ë‹¨ìœ„ ì§‘ê³„ ë°ì´í„° â†’ ìƒíƒœí•™ì  ì˜¤ë¥˜(ecological fallacy) ê°€ëŠ¥ì„±
  - íŠ¸ë¦¬ ëª¨ë¸ íŠ¹ì„±ìƒ ë¹„ì„ í˜•/ìƒí˜¸ì‘ìš© íŒ¨í„´ í¬ì°©
  - ë³¸ ë¶„ì„ì€ 'íƒìƒ‰ì (exploratory)' ì„±ê²©
""")

print(f"\n{'=' * 60}")
print("âœ… ë¶„ì„ ì™„ë£Œ")
print(f"{'=' * 60}")
print(f"ì €ì¥ëœ íŒŒì¼:")
print(f"  1. {fi_path}")
print(f"  2. {shap_path}")
print(f"  3. {pred_path}")
print(f"{'=' * 60}\n")

# =====================================================
# 10. ìš”ì•½ í†µê³„ ì¶œë ¥
# =====================================================
print("ğŸ“ˆ ìš”ì•½ í†µê³„")
print(f"{'=' * 60}")
print(f"ìµœê³  ì¤‘ìš”ë„ ë³€ìˆ˜ (Feature Importance):")
print(f"  â†’ {importance_df.iloc[0]['feature']}")
print(f"     (importance: {importance_df.iloc[0]['importance']:.4f})")
print(f"\nìµœê³  ì¤‘ìš”ë„ ë³€ìˆ˜ (SHAP):")
print(f"  â†’ {shap_summary.iloc[0]['feature']}")
print(f"     (mean |SHAP|: {shap_summary.iloc[0]['mean_abs_shap_value']:.4f})")
print(f"{'=' * 60}\n")
